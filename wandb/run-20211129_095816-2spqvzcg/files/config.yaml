wandb_version: 1

_wandb:
  desc: null
  value:
    cli_version: 0.12.7
    framework: huggingface
    huggingface_version: 4.11.3
    is_jupyter_run: false
    is_kaggle_kernel: false
    m:
    - 1: trainer/global_step
      6:
      - 3
    - 1: learning_rate
      5: 1
      6:
      - 1
    - 1: train_loss_step
      5: 1
      6:
      - 1
    - 1: train_acc_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.word_embeddings\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.position_embeddings\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.token_type_embeddings\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.visual_token_type_embeddings\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.visual_position_embeddings\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.visual_projection\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.visual_projection\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.self\.query\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.self\.query\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.self\.key\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.self\.key\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.self\.value\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.self\.value\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.intermediate\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.intermediate\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.self\.query\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.self\.query\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.self\.key\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.self\.key\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.self\.value\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.self\.value\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.intermediate\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.intermediate\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.self\.query\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.self\.query\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.self\.key\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.self\.key\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.self\.value\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.self\.value\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.intermediate\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.intermediate\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.self\.query\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.self\.query\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.self\.key\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.self\.key\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.self\.value\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.self\.value\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.intermediate\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.intermediate\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.self\.query\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.self\.query\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.self\.key\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.self\.key\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.self\.value\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.self\.value\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.intermediate\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.intermediate\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.self\.query\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.self\.query\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.self\.key\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.self\.key\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.self\.value\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.self\.value\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.intermediate\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.intermediate\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.self\.query\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.self\.query\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.self\.key\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.self\.key\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.self\.value\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.self\.value\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.intermediate\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.intermediate\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.self\.query\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.self\.query\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.self\.key\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.self\.key\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.self\.value\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.self\.value\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.intermediate\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.intermediate\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.self\.query\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.self\.query\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.self\.key\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.self\.key\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.self\.value\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.self\.value\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.intermediate\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.intermediate\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.self\.query\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.self\.query\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.self\.key\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.self\.key\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.self\.value\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.self\.value\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.intermediate\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.intermediate\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.self\.query\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.self\.query\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.self\.key\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.self\.key\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.self\.value\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.self\.value\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.intermediate\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.intermediate\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.self\.query\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.self\.query\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.self\.key\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.self\.key\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.self\.value\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.self\.value\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.intermediate\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.intermediate\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.output\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.output\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.output\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.output\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/text_prediction_head\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/text_prediction_head\.transform\.dense\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/text_prediction_head\.transform\.dense\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/text_prediction_head\.transform\.LayerNorm\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/text_prediction_head\.transform\.LayerNorm\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/text_prediction_head\.decoder\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_img_ft\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_img_ft\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_img_box\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_img_box\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_ln_ft\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_ln_ft\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_ln_box\.weight_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_ln_box\.bias_step
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm_total_step
      5: 1
      6:
      - 1
    - 1: epoch
      5: 1
      6:
      - 1
    - 1: val_loss_step
      5: 1
      6:
      - 1
    - 1: val_acc_step
      5: 1
      6:
      - 1
    - 1: val_loss_epoch
      5: 1
      6:
      - 1
    - 1: val_acc_epoch
      5: 1
      6:
      - 1
    - 1: train_loss_epoch
      5: 1
      6:
      - 1
    - 1: train_acc_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.word_embeddings\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.position_embeddings\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.token_type_embeddings\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.visual_token_type_embeddings\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.visual_position_embeddings\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.visual_projection\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.embeddings\.visual_projection\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.self\.query\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.self\.query\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.self\.key\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.self\.key\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.self\.value\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.self\.value\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.attention\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.intermediate\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.intermediate\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.0\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.self\.query\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.self\.query\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.self\.key\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.self\.key\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.self\.value\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.self\.value\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.attention\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.intermediate\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.intermediate\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.1\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.self\.query\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.self\.query\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.self\.key\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.self\.key\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.self\.value\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.self\.value\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.attention\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.intermediate\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.intermediate\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.2\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.self\.query\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.self\.query\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.self\.key\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.self\.key\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.self\.value\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.self\.value\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.attention\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.intermediate\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.intermediate\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.3\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.self\.query\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.self\.query\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.self\.key\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.self\.key\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.self\.value\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.self\.value\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.attention\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.intermediate\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.intermediate\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.4\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.self\.query\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.self\.query\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.self\.key\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.self\.key\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.self\.value\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.self\.value\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.attention\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.intermediate\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.intermediate\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.5\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.self\.query\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.self\.query\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.self\.key\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.self\.key\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.self\.value\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.self\.value\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.attention\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.intermediate\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.intermediate\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.6\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.self\.query\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.self\.query\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.self\.key\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.self\.key\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.self\.value\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.self\.value\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.attention\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.intermediate\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.intermediate\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.7\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.self\.query\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.self\.query\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.self\.key\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.self\.key\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.self\.value\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.self\.value\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.attention\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.intermediate\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.intermediate\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.8\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.self\.query\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.self\.query\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.self\.key\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.self\.key\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.self\.value\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.self\.value\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.attention\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.intermediate\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.intermediate\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.9\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.self\.query\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.self\.query\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.self\.key\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.self\.key\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.self\.value\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.self\.value\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.attention\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.intermediate\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.intermediate\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.10\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.self\.query\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.self\.query\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.self\.key\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.self\.key\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.self\.value\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.self\.value\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.attention\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.intermediate\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.intermediate\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.output\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.output\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.output\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/model\.encoder\.layer\.11\.output\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/text_prediction_head\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/text_prediction_head\.transform\.dense\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/text_prediction_head\.transform\.dense\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/text_prediction_head\.transform\.LayerNorm\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/text_prediction_head\.transform\.LayerNorm\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/text_prediction_head\.decoder\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_img_ft\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_img_ft\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_img_box\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_img_box\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_ln_ft\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_ln_ft\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_ln_box\.weight_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm/transform_ln_box\.bias_epoch
      5: 1
      6:
      - 1
    - 1: grad_2\.0_norm_total_epoch
      5: 1
      6:
      - 1
    python_version: 3.8.11
    start_time: 1638140296
    t:
      1:
      - 1
      - 9
      - 11
      3:
      - 13
      4: 3.8.11
      5: 0.12.7
      6: 4.11.3
      8:
      - 5
