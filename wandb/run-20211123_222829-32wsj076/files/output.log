/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /media/matt/data21/mmRad/checkpoints/ exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Global seed set to 808
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Beginning training run with #1000 from mscoco_train for #100 epochs...
Start to load Faster-RCNN detected objects from /media/matt/data21/mmRad/img_features/mscoco-train_2017-custom.tsv
Loaded 1000 images in file /media/matt/data21/mmRad/img_features/mscoco-train_2017-custom.tsv in 1 seconds.
Start to load Faster-RCNN detected objects from /media/matt/data21/mmRad/img_features/mscoco-val_2017-custom.tsv
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded 5000 images in file /media/matt/data21/mmRad/img_features/mscoco-val_2017-custom.tsv in 8 seconds.
  | Name                 | Type                       | Params
--------------------------------------------------------------------
0 | model                | VisualBertModel            | 110 M
1 | text_prediction_head | VisualBertLMPredictionHead | 24.1 M
2 | seq_relationship     | Linear                     | 1.5 K
3 | transform_img_ft     | Linear                     | 1.0 M
4 | transform_img_box    | Linear                     | 5.1 K
5 | transform_ln_ft      | LayerNorm                  | 2.0 K
6 | transform_ln_box     | LayerNorm                  | 2.0 K
--------------------------------------------------------------------
135 M     Trainable params
0         Non-trainable params
135 M     Total params
543.152   Total estimated model params size (MB)
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Global seed set to 808
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Epoch 0:  22%|███████████████████████████▌                                                                                                   | 5/23 [00:02<00:07,  2.40it/s, loss=9.57, v_num=j076, train_loss_step=8.950, train_acc_step=12.40]





Epoch 1:  17%|▏| 4/23 [00:02<00:07,  2.45it/s, loss=9.05, v_num=j076, train_loss_step=8.370, train_acc_step=15.30, val_loss_step=8.770, val_acc_step=13.00, val_loss_epoch=8.610, val_acc_epoch=14.90, train_loss_epoch=9.490, train_acc_epoch=1





Epoch 2:  35%|▎| 8/23 [00:03<00:06,  2.47it/s, loss=8.85, v_num=j076, train_loss_step=8.350, train_acc_step=15.20, val_loss_step=8.760, val_acc_step=12.30, val_loss_epoch=8.490, val_acc_epoch=14.40, train_loss_epoch=8.510, train_acc_epoch=1





Epoch 3:  26%|▎| 6/23 [00:02<00:06,  2.45it/s, loss=8.74, v_num=j076, train_loss_step=8.240, train_acc_step=16.40, val_loss_step=8.810, val_acc_step=13.00, val_loss_epoch=8.450, val_acc_epoch=13.90, train_loss_epoch=8.450, train_acc_epoch=1





Epoch 4:  26%|▎| 6/23 [00:02<00:06,  2.44it/s, loss=8.66, v_num=j076, train_loss_step=8.250, train_acc_step=15.60, val_loss_step=8.260, val_acc_step=16.30, val_loss_epoch=8.390, val_acc_epoch=14.50, train_loss_epoch=8.430, train_acc_epoch=1





Epoch 5:  17%|▏| 4/23 [00:02<00:08,  2.33it/s, loss=8.63, v_num=j076, train_loss_step=8.520, train_acc_step=14.20, val_loss_step=8.320, val_acc_step=16.50, val_loss_epoch=8.290, val_acc_epoch=14.70, train_loss_epoch=8.350, train_acc_epoch=1




Epoch 6:  17%|▏| 4/23 [00:02<00:08,  2.34it/s, loss=8.49, v_num=j076, train_loss_step=8.440, train_acc_step=12.60, val_loss_step=8.750, val_acc_step=8.130, val_loss_epoch=8.390, val_acc_epoch=13.70, train_loss_epoch=8.460, train_acc_epoch=1




Epoch 7:  26%|▎| 6/23 [00:02<00:06,  2.48it/s, loss=8.41, v_num=j076, train_loss_step=8.140, train_acc_step=16.10, val_loss_step=8.290, val_acc_step=18.00, val_loss_epoch=8.330, val_acc_epoch=14.30, train_loss_epoch=8.380, train_acc_epoch=1




Epoch 8:  35%|▎| 8/23 [00:03<00:06,  2.48it/s, loss=8.4, v_num=j076, train_loss_step=8.440, train_acc_step=15.30, val_loss_step=8.630, val_acc_step=13.20, val_loss_epoch=8.300, val_acc_epoch=14.50, train_loss_epoch=8.350, train_acc_epoch=14



Epoch 9:  17%|▏| 4/23 [00:02<00:07,  2.41it/s, loss=8.37, v_num=j076, train_loss_step=8.390, train_acc_step=14.60, val_loss_step=8.820, val_acc_step=9.430, val_loss_epoch=8.360, val_acc_epoch=13.90, train_loss_epoch=8.400, train_acc_epoch=1




Epoch 10:  17%|▏| 4/23 [00:02<00:08,  2.36it/s, loss=8.38, v_num=j076, train_loss_step=8.240, train_acc_step=16.40, val_loss_step=8.400, val_acc_step=16.00, val_loss_epoch=8.330, val_acc_epoch=14.20, train_loss_epoch=8.360, train_acc_epoch=







Epoch 11:   4%| | 1/23 [11:33<2:07:06, 346.66s/it, loss=8.38, v_num=j076, train_loss_step=8.470, train_acc_step=13.00, val_loss_step=8.290, val_acc_step=16.40, val_loss_epoch=8.270, val_acc_epoch=14.50, train_loss_epoch=8.310, tr
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
Epoch 11:  13%|▏| 3/23 [11:38<58:12, 174.63s/it, loss=8.37, v_num=j076, train_loss_step=8.150, train_acc_step=18.20, val_loss_step=8.290, val_acc_step=16.40, val_loss_epoch=8.270, val_acc_epoch=14.50, train_loss_epoch=8.310, trai
Validating:   0%|                                                                                                                                                                                             | 0/20 [00:00<?, ?it/s]
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).




Epoch 12:  26%|▎| 6/23 [00:02<00:06,  2.49it/s, loss=8.36, v_num=j076, train_loss_step=8.400, train_acc_step=13.70, val_loss_step=8.310, val_acc_step=16.00, val_loss_epoch=8.320, val_acc_epoch=14.30, train_loss_epoch=8.360, train




Epoch 13:  35%|▎| 8/23 [00:03<00:06,  2.50it/s, loss=8.34, v_num=j076, train_loss_step=8.090, train_acc_step=15.50, val_loss_step=8.540, val_acc_step=11.10, val_loss_epoch=8.290, val_acc_epoch=13.90, train_loss_epoch=8.360, train




Epoch 14:  17%|▏| 4/23 [00:02<00:08,  2.37it/s, loss=8.34, v_num=j076, train_loss_step=8.370, train_acc_step=14.10, val_loss_step=8.500, val_acc_step=12.70, val_loss_epoch=8.260, val_acc_epoch=14.60, train_loss_epoch=8.280, train




Epoch 15:  17%|▏| 4/23 [00:02<00:07,  2.41it/s, loss=8.31, v_num=j076, train_loss_step=8.490, train_acc_step=13.10, val_loss_step=8.420, val_acc_step=14.10, val_loss_epoch=8.370, val_acc_epoch=13.70, train_loss_epoch=8.330, train




Epoch 16:  17%|▏| 4/23 [00:02<00:08,  2.25it/s, loss=8.33, v_num=j076, train_loss_step=8.370, train_acc_step=13.10, val_loss_step=8.710, val_acc_step=11.70, val_loss_epoch=8.310, val_acc_epoch=14.30, train_loss_epoch=8.240, train





Epoch 17:  17%|▏| 4/23 [00:02<00:07,  2.38it/s, loss=8.34, v_num=j076, train_loss_step=8.410, train_acc_step=13.00, val_loss_step=8.300, val_acc_step=15.60, val_loss_epoch=8.220, val_acc_epoch=14.80, train_loss_epoch=8.420, train




Epoch 18:  17%|▏| 4/23 [00:02<00:07,  2.46it/s, loss=8.34, v_num=j076, train_loss_step=8.150, train_acc_step=16.10, val_loss_step=8.620, val_acc_step=11.80, val_loss_epoch=8.290, val_acc_epoch=14.20, train_loss_epoch=8.470, train




Epoch 19:  26%|▎| 6/23 [00:02<00:07,  2.42it/s, loss=8.34, v_num=j076, train_loss_step=8.340, train_acc_step=14.70, val_loss_step=8.430, val_acc_step=13.10, val_loss_epoch=8.270, val_acc_epoch=14.60, train_loss_epoch=8.330, train




Epoch 20:  26%|▎| 6/23 [00:02<00:07,  2.34it/s, loss=8.34, v_num=j076, train_loss_step=8.220, train_acc_step=15.80, val_loss_step=8.750, val_acc_step=10.30, val_loss_epoch=8.310, val_acc_epoch=14.20, train_loss_epoch=8.300, train




Epoch 21:  17%|▏| 4/23 [00:02<00:08,  2.33it/s, loss=8.33, v_num=j076, train_loss_step=8.140, train_acc_step=16.40, val_loss_step=8.560, val_acc_step=13.90, val_loss_epoch=8.280, val_acc_epoch=14.20, train_loss_epoch=8.310, train




Epoch 22:  26%|▎| 6/23 [00:02<00:07,  2.42it/s, loss=8.32, v_num=j076, train_loss_step=8.300, train_acc_step=14.10, val_loss_step=8.390, val_acc_step=12.20, val_loss_epoch=8.370, val_acc_epoch=13.50, train_loss_epoch=8.180, train




Epoch 23:  26%|▎| 6/23 [00:03<00:07,  2.29it/s, loss=8.3, v_num=j076, train_loss_step=8.430, train_acc_step=14.20, val_loss_step=8.540, val_acc_step=11.40, val_loss_epoch=8.270, val_acc_epoch=14.50, train_loss_epoch=8.240, train_




Epoch 24:  26%|▎| 6/23 [00:02<00:06,  2.44it/s, loss=8.28, v_num=j076, train_loss_step=8.300, train_acc_step=16.90, val_loss_step=8.630, val_acc_step=13.10, val_loss_epoch=8.250, val_acc_epoch=14.90, train_loss_epoch=8.330, train




Epoch 25:  35%|▎| 8/23 [00:03<00:06,  2.28it/s, loss=8.3, v_num=j076, train_loss_step=8.160, train_acc_step=15.30, val_loss_step=8.340, val_acc_step=14.20, val_loss_epoch=8.260, val_acc_epoch=14.30, train_loss_epoch=8.330, train_



Epoch 26:  17%|▏| 4/23 [00:02<00:07,  2.38it/s, loss=8.28, v_num=j076, train_loss_step=8.180, train_acc_step=16.50, val_loss_step=8.430, val_acc_step=11.90, val_loss_epoch=8.320, val_acc_epoch=13.90, train_loss_epoch=8.330, train




Epoch 27:  26%|▎| 6/23 [00:02<00:06,  2.53it/s, loss=8.27, v_num=j076, train_loss_step=8.120, train_acc_step=14.40, val_loss_step=8.440, val_acc_step=12.70, val_loss_epoch=8.240, val_acc_epoch=14.60, train_loss_epoch=8.220, train




Epoch 28:  35%|▎| 8/23 [00:03<00:05,  2.53it/s, loss=8.31, v_num=j076, train_loss_step=8.520, train_acc_step=11.60, val_loss_step=8.560, val_acc_step=13.00, val_loss_epoch=8.280, val_acc_epoch=13.90, train_loss_epoch=8.310, train



Epoch 29:  17%|▏| 4/23 [00:02<00:07,  2.45it/s, loss=8.31, v_num=j076, train_loss_step=8.390, train_acc_step=13.40, val_loss_step=8.350, val_acc_step=12.00, val_loss_epoch=8.290, val_acc_epoch=14.20, train_loss_epoch=8.350, train




Epoch 30:  26%|▎| 6/23 [00:03<00:07,  2.31it/s, loss=8.3, v_num=j076, train_loss_step=8.240, train_acc_step=15.10, val_loss_step=8.250, val_acc_step=12.70, val_loss_epoch=8.220, val_acc_epoch=14.40, train_loss_epoch=8.280, train_




Epoch 31:  26%|▎| 6/23 [00:02<00:06,  2.43it/s, loss=8.29, v_num=j076, train_loss_step=8.330, train_acc_step=15.80, val_loss_step=8.590, val_acc_step=11.10, val_loss_epoch=8.230, val_acc_epoch=14.60, train_loss_epoch=8.300, train




Epoch 32:  26%|▎| 6/23 [00:02<00:06,  2.45it/s, loss=8.27, v_num=j076, train_loss_step=8.130, train_acc_step=16.20, val_loss_step=8.360, val_acc_step=15.10, val_loss_epoch=8.210, val_acc_epoch=14.80, train_loss_epoch=8.350, train




Epoch 33:  35%|▎| 8/23 [00:03<00:06,  2.38it/s, loss=8.28, v_num=j076, train_loss_step=8.410, train_acc_step=15.10, val_loss_step=8.660, val_acc_step=11.10, val_loss_epoch=8.260, val_acc_epoch=14.40, train_loss_epoch=8.130, train



Epoch 34:  26%|▎| 6/23 [00:02<00:06,  2.51it/s, loss=8.28, v_num=j076, train_loss_step=8.170, train_acc_step=15.10, val_loss_step=8.410, val_acc_step=15.00, val_loss_epoch=8.290, val_acc_epoch=14.50, train_loss_epoch=8.290, train




Epoch 35:  26%|▎| 6/23 [00:02<00:07,  2.37it/s, loss=8.25, v_num=j076, train_loss_step=8.100, train_acc_step=18.10, val_loss_step=8.260, val_acc_step=13.30, val_loss_epoch=8.240, val_acc_epoch=14.50, train_loss_epoch=8.300, train




Epoch 36:  35%|▎| 8/23 [00:03<00:05,  2.54it/s, loss=8.28, v_num=j076, train_loss_step=8.340, train_acc_step=13.40, val_loss_step=8.350, val_acc_step=15.30, val_loss_epoch=8.250, val_acc_epoch=14.20, train_loss_epoch=8.200, train



Epoch 37:  17%|▏| 4/23 [00:02<00:07,  2.45it/s, loss=8.27, v_num=j076, train_loss_step=8.150, train_acc_step=15.60, val_loss_step=8.650, val_acc_step=11.50, val_loss_epoch=8.260, val_acc_epoch=14.20, train_loss_epoch=8.380, train




Epoch 38:  26%|▎| 6/23 [00:02<00:06,  2.44it/s, loss=8.29, v_num=j076, train_loss_step=8.420, train_acc_step=12.20, val_loss_step=8.270, val_acc_step=15.30, val_loss_epoch=8.260, val_acc_epoch=14.20, train_loss_epoch=8.310, train





