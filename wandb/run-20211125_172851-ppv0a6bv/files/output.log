[34m[1mwandb[39m[22m: logging graph, to disable use `wandb.watch(log_graph=False)`
Global seed set to 808
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Beginning training run with #1024 from mscoco_train for #100 epochs...
Start to load Faster-RCNN detected objects from /media/matt/data21/mmRad/img_features/mscoco-train_2017-custom.tsv
Loaded 1024 images in file /media/matt/data21/mmRad/img_features/mscoco-train_2017-custom.tsv in 1 seconds.
Start to load Faster-RCNN detected objects from /media/matt/data21/mmRad/img_features/mscoco-val_2017-custom.tsv
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded 5000 images in file /media/matt/data21/mmRad/img_features/mscoco-val_2017-custom.tsv in 8 seconds.
  | Name                 | Type                       | Params
--------------------------------------------------------------------
0 | model                | VisualBertModel            | 111 M
1 | text_prediction_head | VisualBertLMPredictionHead | 24.1 M
2 | seq_relationship     | Linear                     | 1.5 K
3 | transform_img_ft     | Linear                     | 2.1 M
4 | transform_img_box    | Linear                     | 10.2 K
5 | transform_ln_ft      | LayerNorm                  | 4.1 K
6 | transform_ln_box     | LayerNorm                  | 4.1 K
--------------------------------------------------------------------
137 M     Trainable params
0         Non-trainable params
137 M     Total params
550.533   Total estimated model params size (MB)
Validation sanity check: 0it [00:00, ?it/s]
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:445: UserWarning: You requested to overfit but enabled val/test dataloader shuffling. We are turning it off for you.
  rank_zero_warn(
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.

Epoch 0:  50%|â–Œ| 1/2 [00:01<00:01,  1.12s/it, loss=10.5, v_num=a6bv, train_loss_step=10.50, train_acc_step=0.000, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=0.305, grad_2.0_norm/model.embeddings.position_embeddings.wei
Validating:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?it/s]
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 41. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
Global seed set to 808
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:605: UserWarning: You requested to overfit but enabled training dataloader shuffling. We are turning off the training dataloader shuffling for you.
  rank_zero_warn(
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.






Epoch 4: 100%|â–ˆ| 2/2 [00:01<00:00,  1.82it/s, loss=8.51, v_num=a6bv, train_loss_step=7.210, train_acc_step=34.00, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=0.328, grad_2.0_norm/model.embeddings.position_embeddings.wei

Epoch 5: 100%|â–ˆ| 2/2 [00:01<00:00,  1.78it/s, loss=8.24, v_num=a6bv, train_loss_step=6.860, train_acc_step=35.30, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=0.444, grad_2.0_norm/model.embeddings.position_embeddings.wei








Epoch 10: 100%|â–ˆ| 2/2 [00:01<00:00,  1.78it/s, loss=7.29, v_num=a6bv, train_loss_step=5.780, train_acc_step=47.00, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=1.120, grad_2.0_norm/model.embeddings.position_embeddings.we

Epoch 11: 100%|â–ˆ| 2/2 [00:01<00:00,  1.74it/s, loss=7.15, v_num=a6bv, train_loss_step=5.670, train_acc_step=46.90, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=1.390, grad_2.0_norm/model.embeddings.position_embeddings.we








Epoch 16: 100%|â–ˆ| 2/2 [00:01<00:00,  1.77it/s, loss=6.7, v_num=a6bv, train_loss_step=5.600, train_acc_step=46.70, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=1.880, grad_2.0_norm/model.embeddings.position_embeddings.wei

Epoch 17: 100%|â–ˆ| 2/2 [00:01<00:00,  1.82it/s, loss=6.64, v_num=a6bv, train_loss_step=5.560, train_acc_step=49.40, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=1.280, grad_2.0_norm/model.embeddings.position_embeddings.we



Epoch 19: 100%|â–ˆ| 2/2 [00:01<00:00,  1.80it/s, loss=6.49, v_num=a6bv, train_loss_step=4.970, train_acc_step=50.90, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=1.670, grad_2.0_norm/model.embeddings.position_embeddings.we

Epoch 20: 100%|â–ˆ| 2/2 [00:01<00:00,  1.82it/s, loss=6.21, v_num=a6bv, train_loss_step=4.940, train_acc_step=53.70, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=2.950, grad_2.0_norm/model.embeddings.position_embeddings.we








Epoch 25: 100%|â–ˆ| 2/2 [00:01<00:00,  1.78it/s, loss=5.5, v_num=a6bv, train_loss_step=4.850, train_acc_step=55.00, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=2.640, grad_2.0_norm/model.embeddings.position_embeddings.wei

Epoch 26: 100%|â–ˆ| 2/2 [00:01<00:00,  1.79it/s, loss=5.43, v_num=a6bv, train_loss_step=4.770, train_acc_step=54.40, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=1.060, grad_2.0_norm/model.embeddings.position_embeddings.we













Epoch 34: 100%|â–ˆ| 2/2 [00:01<00:00,  1.67it/s, loss=4.89, v_num=a6bv, train_loss_step=4.450, train_acc_step=58.10, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=1.750, grad_2.0_norm/model.embeddings.position_embeddings.we












Epoch 41: 100%|â–ˆ| 2/2 [00:01<00:00,  1.72it/s, loss=4.51, v_num=a6bv, train_loss_step=3.960, train_acc_step=63.50, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=2.390, grad_2.0_norm/model.embeddings.position_embeddings.we










Epoch 47: 100%|â–ˆ| 2/2 [00:01<00:00,  1.75it/s, loss=4.25, v_num=a6bv, train_loss_step=3.950, train_acc_step=61.40, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=7.850, grad_2.0_norm/model.embeddings.position_embeddings.we

Epoch 48: 100%|â–ˆ| 2/2 [00:01<00:00,  1.80it/s, loss=4.31, v_num=a6bv, train_loss_step=5.830, train_acc_step=42.80, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=13.40, grad_2.0_norm/model.embeddings.position_embeddings.we










Epoch 54: 100%|â–ˆ| 2/2 [00:01<00:00,  1.78it/s, loss=4.21, v_num=a6bv, train_loss_step=4.170, train_acc_step=60.90, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=4.240, grad_2.0_norm/model.embeddings.position_embeddings.we










Epoch 60: 100%|â–ˆ| 2/2 [00:01<00:00,  1.73it/s, loss=4.21, v_num=a6bv, train_loss_step=4.880, train_acc_step=50.40, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=9.080, grad_2.0_norm/model.embeddings.position_embeddings.we



Epoch 62: 100%|â–ˆ| 2/2 [00:01<00:00,  1.84it/s, loss=4.22, v_num=a6bv, train_loss_step=3.910, train_acc_step=63.50, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=3.490, grad_2.0_norm/model.embeddings.position_embeddings.we














Epoch 70: 100%|â–ˆ| 2/2 [00:01<00:00,  1.78it/s, loss=4.41, v_num=a6bv, train_loss_step=5.320, train_acc_step=46.20, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=4.000, grad_2.0_norm/model.embeddings.position_embeddings.we










Epoch 76: 100%|â–ˆ| 2/2 [00:01<00:00,  1.84it/s, loss=4.84, v_num=a6bv, train_loss_step=4.630, train_acc_step=52.80, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=6.150, grad_2.0_norm/model.embeddings.position_embeddings.we

Epoch 77: 100%|â–ˆ| 2/2 [00:01<00:00,  1.81it/s, loss=4.96, v_num=a6bv, train_loss_step=6.380, train_acc_step=33.30, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=46.60, grad_2.0_norm/model.embeddings.position_embeddings.we








Epoch 82: 100%|â–ˆ| 2/2 [00:01<00:00,  1.76it/s, loss=5.25, v_num=a6bv, train_loss_step=5.220, train_acc_step=46.90, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=9.810, grad_2.0_norm/model.embeddings.position_embeddings.we

Epoch 83: 100%|â–ˆ| 2/2 [00:01<00:00,  1.73it/s, loss=5.32, v_num=a6bv, train_loss_step=5.560, train_acc_step=43.40, grad_2.0_norm/model.embeddings.word_embeddings.weight_step=7.730, grad_2.0_norm/model.embeddings.position_embeddings.we


