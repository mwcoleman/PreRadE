
Beginning training run with #5120 from mscoco_train for #20 epochs...
[34m[1mwandb[39m[22m: logging graph, to disable use `wandb.watch(log_graph=False)`
Global seed set to 808
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  rank_zero_deprecation(
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Validation sanity check:   0%|                                                                                                                                                                 | 0/2 [00:00<?, ?it/s]
  | Name                 | Type                       | Params
--------------------------------------------------------------------
0 | model                | VisualBertModel            | 111 M
1 | text_prediction_head | VisualBertLMPredictionHead | 24.1 M
2 | seq_relationship     | Linear                     | 1.5 K
3 | transform_img_ft     | Linear                     | 2.1 M
4 | transform_img_box    | Linear                     | 10.2 K
5 | transform_ln_ft      | LayerNorm                  | 4.1 K
6 | transform_ln_box     | LayerNorm                  | 4.1 K
--------------------------------------------------------------------
137 M     Trainable params
0         Non-trainable params
137 M     Total params
550.533   Total estimated model params size (MB)
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /media/matt/data21/mmRad/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")







Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                 | 20/40 [00:15<00:15,  1.31it/s, loss=8.87, v_num=zmqy, train_loss_step=7.040, train_acc_step=34.10]











Epoch 1:  50%|â–Œ| 20/40 [00:15<00:15,  1.32it/s, loss=6.32, v_num=zmqy, train_loss_step=5.710, train_acc_step=48.00, val_loss_step=6.880, val_acc_step=36.60, val_loss_epoch=7.090, val_acc_epoch=33.20, train_loss_ep













Epoch 2:  55%|â–Œ| 22/40 [00:16<00:13,  1.31it/s, loss=5.99, v_num=zmqy, train_loss_step=5.380, train_acc_step=45.50, val_loss_step=10.00, val_acc_step=9.880, val_loss_epoch=10.20, val_acc_epoch=9.150, train_loss_ep











Epoch 3:  50%|â–Œ| 20/40 [00:15<00:15,  1.30it/s, loss=6.04, v_num=zmqy, train_loss_step=6.530, train_acc_step=31.30, val_loss_step=4.980, val_acc_step=51.10, val_loss_epoch=5.600, val_acc_epoch=43.60, train_loss_ep












Epoch 4:  50%|â–Œ| 20/40 [00:15<00:15,  1.31it/s, loss=8.39, v_num=zmqy, train_loss_step=8.390, train_acc_step=13.20, val_loss_step=6.630, val_acc_step=34.60, val_loss_epoch=7.250, val_acc_epoch=26.70, train_loss_ep












Epoch 5:  55%|â–Œ| 22/40 [00:16<00:13,  1.32it/s, loss=8.34, v_num=zmqy, train_loss_step=8.160, train_acc_step=16.30, val_loss_step=8.730, val_acc_step=7.660, val_loss_epoch=8.320, val_acc_epoch=13.80, train_loss_ep











Epoch 6:  55%|â–Œ| 22/40 [00:16<00:13,  1.32it/s, loss=8.37, v_num=zmqy, train_loss_step=8.280, train_acc_step=14.60, val_loss_step=8.460, val_acc_step=11.70, val_loss_epoch=8.280, val_acc_epoch=14.10, train_loss_ep











Epoch 7:  55%|â–Œ| 22/40 [00:16<00:13,  1.33it/s, loss=8.38, v_num=zmqy, train_loss_step=8.250, train_acc_step=15.80, val_loss_step=8.440, val_acc_step=13.20, val_loss_epoch=8.210, val_acc_epoch=14.50, train_loss_ep











Epoch 8:  55%|â–Œ| 22/40 [00:16<00:13,  1.33it/s, loss=8.34, v_num=zmqy, train_loss_step=8.260, train_acc_step=16.30, val_loss_step=8.380, val_acc_step=13.30, val_loss_epoch=8.280, val_acc_epoch=13.70, train_loss_ep

