Beginning training run with #1000 from mscoco_train for #100 epochs...
Start to load Faster-RCNN detected objects from /media/matt/data21/mmRad/img_features/mscoco-train_2017-custom.tsv
Global seed set to 808
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Loaded 1000 images in file /media/matt/data21/mmRad/img_features/mscoco-train_2017-custom.tsv in 1 seconds.
Start to load Faster-RCNN detected objects from /media/matt/data21/mmRad/img_features/mscoco-val_2017-custom.tsv
Loaded 5000 images in file /media/matt/data21/mmRad/img_features/mscoco-val_2017-custom.tsv in 7 seconds.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Epoch 0:   0%|                                                                                                                                                                                                 | 0/23 [00:00<00:00, 3597.17it/s]
  | Name                 | Type                       | Params
--------------------------------------------------------------------
0 | model                | VisualBertModel            | 110 M
1 | text_prediction_head | VisualBertLMPredictionHead | 24.1 M
2 | seq_relationship     | Linear                     | 1.5 K
3 | transform_img_ft     | Linear                     | 1.0 M
4 | transform_img_box    | Linear                     | 5.1 K
5 | transform_ln_ft      | LayerNorm                  | 2.0 K
6 | transform_ln_box     | LayerNorm                  | 2.0 K
--------------------------------------------------------------------
135 M     Trainable params
0         Non-trainable params
135 M     Total params
543.152   Total estimated model params size (MB)
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Global seed set to 808
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 0:  22%|███████████████████████████▌                                                                                                   | 5/23 [00:01<00:05,  3.14it/s, loss=9.83, v_num=cp0h, train_loss_step=9.360, train_acc_step=10.50]




Epoch 1:  43%|▍| 10/23 [00:02<00:03,  3.74it/s, loss=9.35, v_num=cp0h, train_loss_step=8.690, train_acc_step=13.00, val_loss_step=9.150, val_acc_step=11.20, val_loss_epoch=8.930, val_acc_epoch=12.60, train_loss_epoch=9.800, train_acc_epoch=



Epoch 2:  26%|▎| 6/23 [00:02<00:05,  3.30it/s, loss=9.08, v_num=cp0h, train_loss_step=8.390, train_acc_step=12.70, val_loss_step=8.760, val_acc_step=10.40, val_loss_epoch=8.610, val_acc_epoch=12.20, train_loss_epoch=8.850, train_acc_epoch=1




Epoch 3:  43%|▍| 10/23 [00:02<00:03,  3.76it/s, loss=8.89, v_num=cp0h, train_loss_step=8.090, train_acc_step=14.70, val_loss_step=8.650, val_acc_step=11.30, val_loss_epoch=8.390, val_acc_epoch=11.90, train_loss_epoch=8.540, train_acc_epoch=



Epoch 4:  26%|▎| 6/23 [00:02<00:05,  3.27it/s, loss=8.74, v_num=cp0h, train_loss_step=8.060, train_acc_step=19.60, val_loss_step=8.180, val_acc_step=18.80, val_loss_epoch=8.170, val_acc_epoch=17.50, train_loss_epoch=8.320, train_acc_epoch=1




Epoch 5:  26%|▎| 6/23 [00:02<00:05,  3.35it/s, loss=8.62, v_num=cp0h, train_loss_step=8.120, train_acc_step=18.80, val_loss_step=7.980, val_acc_step=21.40, val_loss_epoch=7.920, val_acc_epoch=20.00, train_loss_epoch=8.130, train_acc_epoch=1



Epoch 5: 100%|█| 23/23 [00:21<00:00,  1.12it/s, loss=8.62, v_num=cp0h, train_loss_step=8.120, train_acc_step=18.80, val_loss_step=8.220, val_acc_step=12.70, val_loss_epoch=7.890, val_acc_epoch=19.10, train_loss_epoch=8.130, train_acc_epoch=
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1051: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...

Epoch 6:   0%| | 0/23 [00:00<00:00, 2601.93it/s, loss=8.62, v_num=cp0h, train_loss_step=8.120, train_acc_step=18.80, val_loss_step=8.220, val_acc_step=12.70, val_loss_epoch=7.890, val_acc_epoch=19.10, train_loss_epoch=8.130, train_acc_epoch