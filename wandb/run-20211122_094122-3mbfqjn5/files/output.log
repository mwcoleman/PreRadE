GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Start to load Faster-RCNN detected objects from /media/matt/data21/mmRad/img_features/mscoco-train_2017-custom.tsv
Loaded 5000 images in file /media/matt/data21/mmRad/img_features/mscoco-train_2017-custom.tsv in 8 seconds.
Start to load Faster-RCNN detected objects from /media/matt/data21/mmRad/img_features/mscoco-val_2017-custom.tsv
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded 5000 images in file /media/matt/data21/mmRad/img_features/mscoco-val_2017-custom.tsv in 7 seconds.
  | Name                 | Type                       | Params
--------------------------------------------------------------------
0 | model                | VisualBertModel            | 110 M
1 | text_prediction_head | VisualBertLMPredictionHead | 24.1 M
2 | seq_relationship     | Linear                     | 1.5 K
3 | transform_img_ft     | Linear                     | 1.0 M
4 | transform_img_box    | Linear                     | 5.1 K
5 | transform_ln_ft      | LayerNorm                  | 2.0 K
6 | transform_ln_box     | LayerNorm                  | 2.0 K
--------------------------------------------------------------------
135 M     Trainable params
0         Non-trainable params
135 M     Total params
543.152   Total estimated model params size (MB)
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Epoch 0:   5%|███▌                                                                   | 2/40 [00:01<00:13,  2.72it/s, loss=10.1, v_num=qjn5, train_loss_step=9.780, train_acc_step=9.660]
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.




Epoch 0:  60%|██████████████████████████████████████████                            | 24/40 [00:11<00:07,  2.23it/s, loss=8.67, v_num=qjn5, train_loss_step=8.140, train_acc_step=18.50]







Epoch 1:  50%|▌| 20/40 [00:10<00:10,  1.98it/s, loss=7.67, v_num=qjn5, train_loss_step=7.750, train_acc_step=21.00, val_loss_step=8.250, val_acc_step=18.30, val_loss_epoch=7.920, val_a








Epoch 2:  60%|▌| 24/40 [00:11<00:07,  2.19it/s, loss=7.25, v_num=qjn5, train_loss_step=6.980, train_acc_step=28.00, val_loss_step=7.420, val_acc_step=24.80, val_loss_epoch=7.300, val_a








Epoch 3:  70%|▋| 28/40 [00:12<00:05,  2.35it/s, loss=7.15, v_num=qjn5, train_loss_step=7.060, train_acc_step=26.30, val_loss_step=7.290, val_acc_step=24.90, val_loss_epoch=7.100, val_a







Epoch 4:  60%|▌| 24/40 [00:11<00:07,  2.19it/s, loss=6.99, v_num=qjn5, train_loss_step=7.030, train_acc_step=27.20, val_loss_step=7.170, val_acc_step=23.70, val_loss_epoch=7.020, val_a








Epoch 5:  65%|▋| 26/40 [00:11<00:06,  2.26it/s, loss=6.85, v_num=qjn5, train_loss_step=6.760, train_acc_step=30.70, val_loss_step=7.100, val_acc_step=27.70, val_loss_epoch=6.870, val_a







Epoch 6:  55%|▌| 22/40 [00:10<00:08,  2.09it/s, loss=6.78, v_num=qjn5, train_loss_step=6.630, train_acc_step=33.50, val_loss_step=6.890, val_acc_step=30.90, val_loss_epoch=6.750, val_a








Epoch 7:  65%|▋| 26/40 [00:11<00:06,  2.29it/s, loss=6.64, v_num=qjn5, train_loss_step=6.500, train_acc_step=34.30, val_loss_step=6.580, val_acc_step=33.00, val_loss_epoch=6.630, val_a








Epoch 8:  70%|▋| 28/40 [00:12<00:05,  2.38it/s, loss=6.53, v_num=qjn5, train_loss_step=6.110, train_acc_step=38.90, val_loss_step=6.450, val_acc_step=35.50, val_loss_epoch=6.530, val_a







Epoch 9:  60%|▌| 24/40 [00:11<00:07,  2.19it/s, loss=6.52, v_num=qjn5, train_loss_step=6.540, train_acc_step=33.60, val_loss_step=6.250, val_acc_step=34.30, val_loss_epoch=6.480, val_a








Epoch 10:  65%|▋| 26/40 [00:11<00:06,  2.28it/s, loss=6.47, v_num=qjn5, train_loss_step=6.580, train_acc_step=33.70, val_loss_step=6.430, val_acc_step=34.90, val_loss_epoch=6.520, val_







Epoch 11:  55%|▌| 22/40 [00:10<00:08,  2.12it/s, loss=6.45, v_num=qjn5, train_loss_step=6.680, train_acc_step=31.90, val_loss_step=6.450, val_acc_step=34.30, val_loss_epoch=6.480, val_








Epoch 12:  70%|▋| 28/40 [00:11<00:04,  2.47it/s, loss=6.37, v_num=qjn5, train_loss_step=6.120, train_acc_step=37.40, val_loss_step=6.340, val_acc_step=35.50, val_loss_epoch=6.430, val_







Epoch 13:  70%|▋| 28/40 [00:11<00:04,  2.50it/s, loss=6.34, v_num=qjn5, train_loss_step=6.220, train_acc_step=37.20, val_loss_step=6.510, val_acc_step=34.20, val_loss_epoch=6.330, val_







Epoch 14:  65%|▋| 26/40 [00:11<00:05,  2.41it/s, loss=6.29, v_num=qjn5, train_loss_step=6.730, train_acc_step=32.60, val_loss_step=6.500, val_acc_step=34.80, val_loss_epoch=6.230, val_







Epoch 15:  65%|▋| 26/40 [00:11<00:05,  2.43it/s, loss=6.26, v_num=qjn5, train_loss_step=6.340, train_acc_step=34.60, val_loss_step=6.210, val_acc_step=36.50, val_loss_epoch=6.300, val_







Epoch 16:  60%|▌| 24/40 [00:11<00:07,  2.25it/s, loss=6.27, v_num=qjn5, train_loss_step=5.780, train_acc_step=40.70, val_loss_step=6.210, val_acc_step=36.40, val_loss_epoch=6.240, val_








Epoch 17:  65%|▋| 26/40 [00:11<00:06,  2.27it/s, loss=6.32, v_num=qjn5, train_loss_step=6.160, train_acc_step=36.60, val_loss_step=6.380, val_acc_step=34.90, val_loss_epoch=6.290, val_








Epoch 18:  70%|▋| 28/40 [00:12<00:05,  2.34it/s, loss=6.26, v_num=qjn5, train_loss_step=6.140, train_acc_step=37.30, val_loss_step=6.260, val_acc_step=36.90, val_loss_epoch=6.290, val_







Epoch 19:  60%|▌| 24/40 [00:11<00:07,  2.17it/s, loss=6.24, v_num=qjn5, train_loss_step=6.420, train_acc_step=33.60, val_loss_step=6.470, val_acc_step=33.80, val_loss_epoch=6.180, val_



Epoch 19: 100%|█| 40/40 [00:17<00:00,  2.37it/s, loss=6.24, v_num=qjn5, train_loss_step=6.420, train_acc_step=33.60, val_loss_step=6.040, val_acc_step=38.10, val_loss_epoch=6.140, val_