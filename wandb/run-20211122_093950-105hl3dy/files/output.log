
Epoch 0:   2%|█▊                                                                     | 1/40 [00:00<00:10,  3.56it/s, loss=10.5, v_num=l3dy, train_loss_step=10.50, train_acc_step=0.000]
  | Name                 | Type                       | Params
--------------------------------------------------------------------
0 | model                | VisualBertModel            | 110 M
1 | text_prediction_head | VisualBertLMPredictionHead | 24.1 M
2 | seq_relationship     | Linear                     | 1.5 K
3 | transform_img_ft     | Linear                     | 1.0 M
4 | transform_img_box    | Linear                     | 5.1 K
5 | transform_ln_ft      | LayerNorm                  | 2.0 K
6 | transform_ln_box     | LayerNorm                  | 2.0 K
--------------------------------------------------------------------
135 M     Trainable params
0         Non-trainable params
135 M     Total params
543.152   Total estimated model params size (MB)
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.




Epoch 0:  55%|██████████████████████████████████████▌                               | 22/40 [00:10<00:08,  2.12it/s, loss=8.76, v_num=l3dy, train_loss_step=7.750, train_acc_step=24.50]








Epoch 1:  75%|▊| 30/40 [00:12<00:03,  2.56it/s, loss=7.72, v_num=l3dy, train_loss_step=7.300, train_acc_step=27.70, val_loss_step=7.920, val_acc_step=23.10, val_loss_epoch=7.960, val_a







Epoch 2:  70%|▋| 28/40 [00:11<00:04,  2.45it/s, loss=7.3, v_num=l3dy, train_loss_step=7.390, train_acc_step=23.40, val_loss_step=7.360, val_acc_step=26.10, val_loss_epoch=7.400, val_ac







Epoch 3:  55%|▌| 22/40 [00:10<00:08,  2.11it/s, loss=7.09, v_num=l3dy, train_loss_step=7.210, train_acc_step=25.90, val_loss_step=7.420, val_acc_step=22.20, val_loss_epoch=7.150, val_a
Validating:  10%|█████████████▌                                                                                                                          | 2/20 [00:00<00:04,  4.03it/s]
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1051: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")