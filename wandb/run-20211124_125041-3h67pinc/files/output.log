/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /media/matt/data21/mmRad/checkpoints/ exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Global seed set to 808
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Beginning training run with #1024 from mscoco_train for #100 epochs...
Start to load Faster-RCNN detected objects from /media/matt/data21/mmRad/img_features/mscoco-train_2017-custom.tsv
Loaded 1024 images in file /media/matt/data21/mmRad/img_features/mscoco-train_2017-custom.tsv in 1 seconds.
Start to load Faster-RCNN detected objects from /media/matt/data21/mmRad/img_features/mscoco-val_2017-custom.tsv
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded 5000 images in file /media/matt/data21/mmRad/img_features/mscoco-val_2017-custom.tsv in 8 seconds.
  | Name                 | Type                       | Params
--------------------------------------------------------------------
0 | model                | VisualBertModel            | 111 M
1 | text_prediction_head | VisualBertLMPredictionHead | 24.1 M
2 | seq_relationship     | Linear                     | 1.5 K
3 | transform_img_ft     | Linear                     | 2.1 M
4 | transform_img_box    | Linear                     | 10.2 K
5 | transform_ln_ft      | LayerNorm                  | 4.1 K
6 | transform_ln_box     | LayerNorm                  | 4.1 K
--------------------------------------------------------------------
137 M     Trainable params
0         Non-trainable params
137 M     Total params
550.533   Total estimated model params size (MB)
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Validation sanity check:  50%|████████████████████████                        | 1/2 [00:00<00:00,  2.45it/s]
Global seed set to 808
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.

Epoch 0:  25%|▎| 6/24 [00:03<00:07,  2.25it/s, loss=10, v_num=pinc, train_loss_step=9.290, train_acc_step=17





Epoch 1:  17%|▏| 4/24 [00:02<00:10,  1.86it/s, loss=9.42, v_num=pinc, train_loss_step=8.710, train_acc_step=





Epoch 2:  25%|▎| 6/24 [00:03<00:07,  2.28it/s, loss=9.15, v_num=pinc, train_loss_step=8.730, train_acc_step=11.10, val_loss_step





Epoch 3:  17%|▏| 4/24 [00:02<00:10,  1.87it/s, loss=8.99, v_num=pinc, train_loss_step=8.450, train_acc_step=13.40, val_loss_step





Epoch 4:  33%|▎| 8/24 [00:03<00:06,  2.34it/s, loss=8.9, v_num=pinc, train_loss_step=8.550, train_acc_step=13.30, val_loss_step=




Epoch 5:  33%|▎| 8/24 [00:03<00:06,  2.33it/s, loss=8.6, v_num=pinc, train_loss_step=8.630, train_acc_step=13.10, val_loss_step=




Epoch 6:  33%|▎| 8/24 [00:03<00:07,  2.28it/s, loss=8.52, v_num=pinc, train_loss_step=8.370, train_acc_step=14.70, val_loss_step





Epoch 7:  42%|▍| 10/24 [00:04<00:05,  2.38it/s, loss=8.46, v_num=pinc, train_loss_step=8.000, train_acc_step=19.30, val_loss_ste



Epoch 8:  17%|▏| 4/24 [00:02<00:10,  1.86it/s, loss=8.44, v_num=pinc, train_loss_step=8.520, train_acc_step=13.80, val_loss_step





Epoch 9:  33%|▎| 8/24 [00:03<00:06,  2.33it/s, loss=8.42, v_num=pinc, train_loss_step=8.310, train_acc_step=15.20, val_loss_step





Epoch 10:  17%|▏| 4/24 [00:02<00:10,  1.86it/s, loss=8.37, v_num=pinc, train_loss_step=8.160, train_acc_step=16.50, val_loss_ste




Epoch 11:  25%|▎| 6/24 [00:03<00:07,  2.29it/s, loss=8.34, v_num=pinc, train_loss_step=8.350, train_acc_step=15.20, val_loss_ste




Epoch 12:  17%|▏| 4/24 [00:02<00:10,  1.86it/s, loss=8.33, v_num=pinc, train_loss_step=8.320, train_acc_step=14.00, val_loss_ste




Epoch 13:  17%|▏| 4/24 [00:02<00:10,  1.86it/s, loss=8.32, v_num=pinc, train_loss_step=8.410, train_acc_step=14.80, val_loss_ste





Epoch 14:  25%|▎| 6/24 [00:03<00:07,  2.26it/s, loss=8.3, v_num=pinc, train_loss_step=8.430, train_acc_step=12.60, val_loss_step




Epoch 15:  25%|▎| 6/24 [00:03<00:07,  2.27it/s, loss=8.33, v_num=pinc, train_loss_step=8.380, train_acc_step=14.00, val_loss_ste




Epoch 16:  25%|▎| 6/24 [00:03<00:07,  2.28it/s, loss=8.34, v_num=pinc, train_loss_step=8.340, train_acc_step=16.60, val_loss_ste




Epoch 17:  25%|▎| 6/24 [00:03<00:07,  2.26it/s, loss=8.34, v_num=pinc, train_loss_step=8.280, train_acc_step=15.30, val_loss_ste




Epoch 18:  25%|▎| 6/24 [00:03<00:07,  2.29it/s, loss=8.38, v_num=pinc, train_loss_step=8.580, train_acc_step=12.30, val_loss_ste




Epoch 19:  25%|▎| 6/24 [00:03<00:07,  2.27it/s, loss=8.39, v_num=pinc, train_loss_step=8.240, train_acc_step=16.20, val_loss_ste




Epoch 20:  25%|▎| 6/24 [00:03<00:07,  2.27it/s, loss=8.39, v_num=pinc, train_loss_step=8.150, train_acc_step=16.60, val_loss_ste




Epoch 21:  25%|▎| 6/24 [00:03<00:08,  2.22it/s, loss=8.4, v_num=pinc, train_loss_step=8.430, train_acc_step=12.90, val_loss_step






Epoch 22:  12%|▏| 3/24 [00:18<01:37,  4.63s/it, loss=8.4, v_num=pinc, train_loss_step=8.270, train_acc_st
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
Epoch 22:  17%|▏| 4/24 [22:44<1:30:59, 272.99s/it, loss=8.4, v_num=pinc, train_loss_step=8.390, train_acc
