Beginning training run with #1024 from mscoco_train for #100 epochs...
Start to load Faster-RCNN detected objects from /media/matt/data21/mmRad/img_features/mscoco-train_2017-custom.tsv
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /media/matt/data21/mmRad/checkpoints/ exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Global seed set to 808
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Loaded 1024 images in file /media/matt/data21/mmRad/img_features/mscoco-train_2017-custom.tsv in 1 seconds.
Start to load Faster-RCNN detected objects from /media/matt/data21/mmRad/img_features/mscoco-val_2017-custom.tsv
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded 5000 images in file /media/matt/data21/mmRad/img_features/mscoco-val_2017-custom.tsv in 8 seconds.
Validation sanity check:   0%|                                                                                                                                                                                 | 0/2 [00:00<?, ?it/s]
  | Name                 | Type                       | Params
--------------------------------------------------------------------
0 | model                | VisualBertModel            | 111 M
1 | text_prediction_head | VisualBertLMPredictionHead | 24.1 M
2 | seq_relationship     | Linear                     | 1.5 K
3 | transform_img_ft     | Linear                     | 2.1 M
4 | transform_img_box    | Linear                     | 10.2 K
5 | transform_ln_ft      | LayerNorm                  | 4.1 K
6 | transform_ln_box     | LayerNorm                  | 4.1 K
--------------------------------------------------------------------
137 M     Trainable params
0         Non-trainable params
137 M     Total params
550.533   Total estimated model params size (MB)
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Global seed set to 808
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.

Epoch 0:  17%|███████████████████▌                                                                                                 | 4/24 [00:02<00:11,  1.72it/s, loss=9.1, v_num=bh2c, train_loss_step=7.980, train_acc_step=23.00]





Epoch 1:  25%|▎| 6/24 [00:03<00:08,  2.24it/s, loss=8.18, v_num=bh2c, train_loss_step=6.870, train_acc_step=32.50, val_loss_step=7.910, val_acc_step=21.10, val_loss_epoch=7.670, val_acc_epoch=23.10, train_loss_epoch=9.110, train_






Epoch 2:  33%|▎| 8/24 [00:04<00:07,  2.24it/s, loss=7.66, v_num=bh2c, train_loss_step=6.690, train_acc_step=37.00, val_loss_step=7.410, val_acc_step=29.40, val_loss_epoch=6.950, val_acc_epoch=33.90, train_loss_epoch=7.290, train_





Epoch 3:  17%|▏| 4/24 [00:02<00:11,  1.82it/s, loss=7.3, v_num=bh2c, train_loss_step=6.180, train_acc_step=41.80, val_loss_step=6.700, val_acc_step=35.00, val_loss_epoch=6.360, val_acc_epoch=38.10, train_loss_epoch=6.630, train_a





Epoch 4:  25%|▎| 6/24 [00:03<00:08,  2.22it/s, loss=7.15, v_num=bh2c, train_loss_step=8.740, train_acc_step=19.90, val_loss_step=6.280, val_acc_step=38.30, val_loss_epoch=6.150, val_acc_epoch=39.30, train_loss_epoch=6.200, train_




Epoch 5:  17%|▏| 4/24 [00:02<00:10,  1.82it/s, loss=6.55, v_num=bh2c, train_loss_step=6.210, train_acc_step=40.30, val_loss_step=6.420, val_acc_step=39.30, val_loss_epoch=6.150, val_acc_epoch=39.70, train_loss_epoch=6.530, train_





Epoch 6:  25%|▎| 6/24 [00:03<00:08,  2.07it/s, loss=6.26, v_num=bh2c, train_loss_step=5.790, train_acc_step=42.90, val_loss_step=6.430, val_acc_step=34.90, val_loss_epoch=6.140, val_acc_epoch=39.10, train_loss_epoch=6.070, train_





Epoch 7:  25%|▎| 6/24 [00:03<00:08,  2.13it/s, loss=6.05, v_num=bh2c, train_loss_step=4.990, train_acc_step=49.70, val_loss_step=5.860, val_acc_step=43.10, val_loss_epoch=6.000, val_acc_epoch=40.00, train_loss_epoch=5.830, train_


Validating:  20%|███████████████████████████████▏                                                                                                                            | 4/20 [02:37<16:28, 61.76s/it]
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).




Epoch 8:  33%|▎| 8/24 [00:03<00:07,  2.28it/s, loss=6.16, v_num=bh2c, train_loss_step=7.050, train_acc_step=26.60, val_loss_step=6.000, val_acc_step=41.20, val_loss_epoch=6.130, val_acc_epoch=39.10, train




Epoch 9:  25%|▎| 6/24 [00:03<00:08,  2.21it/s, loss=6.1, v_num=bh2c, train_loss_step=6.070, train_acc_step=39.30, val_loss_step=6.430, val_acc_step=34.30, val_loss_epoch=6.520, val_acc_epoch=33.70, train_




Epoch 10:  25%|▎| 6/24 [00:03<00:08,  2.15it/s, loss=6.02, v_num=bh2c, train_loss_step=5.290, train_acc_step=46.10, val_loss_step=6.920, val_acc_step=32.20, val_loss_epoch=6.100, val_acc_epoch=38.70, trai




Epoch 11:  17%|▏| 4/24 [00:02<00:10,  1.83it/s, loss=6.02, v_num=bh2c, train_loss_step=5.920, train_acc_step=39.90, val_loss_step=6.590, val_acc_step=36.80, val_loss_epoch=6.110, val_acc_epoch=38.40, trai






Epoch 12:  42%|▍| 10/24 [00:06<00:08,  1.64it/s, loss=6.12, v_num=bh2c, train_loss_step=5.970, train_acc_step=40.20, val_loss_step=6.770, val_acc_step=30.90, val_loss_epoch=6.120, val_acc_epoch=37.10, tra





Epoch 13:  25%|▎| 6/24 [00:05<00:14,  1.23it/s, loss=5.94, v_num=bh2c, train_loss_step=5.820, train_acc_step=40.70, val_loss_step=6.890, val_acc_step=32.20, val_loss_epoch=6.070, val_acc_epoch=38.50, trai






Epoch 14:  33%|▎| 8/24 [00:06<00:11,  1.35it/s, loss=5.96, v_num=bh2c, train_loss_step=6.600, train_acc_step=32.90, val_loss_step=6.510, val_acc_step=33.90, val_loss_epoch=6.510, val_acc_epoch=32.10, trai






Epoch 15:  33%|▎| 8/24 [00:06<00:11,  1.33it/s, loss=6.02, v_num=bh2c, train_loss_step=6.050, train_acc_step=38.60, val_loss_step=6.730, val_acc_step=33.50, val_loss_epoch=6.150, val_acc_epoch=37.80, trai






Epoch 16:   4%| | 1/24 [10:42<2:03:11, 321.37s/it, loss=6.02, v_num=bh2c, train_loss_step=6.050, train_acc_step=38.60, val_loss_step=6.950, val_acc_step=31.70, val_loss_epoch=6.280, val_acc_epoch=36.40, t
/home/matt/anaconda3/envs/lxmert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).

Epoch 16:  21%|▏| 5/24 [10:46<34:08, 107.83s/it, loss=6.09, v_num=bh2c, train_loss_step=6.150, train_acc_step=36.80, val_loss_step=6.950, val_acc_step=31.70, val_loss_epoch=6.280, val_acc_epoch=36.40, tra






Epoch 17:  21%|▏| 5/24 [00:05<00:17,  1.11it/s, loss=6.13, v_num=bh2c, train_loss_step=6.210, train_acc_step=35.20, val_loss_step=6.490, val_acc_step=32.20, val_loss_epoch=6.400, val_acc_epoch=34.40, trai






Epoch 18:  21%|▏| 5/24 [00:05<00:17,  1.10it/s, loss=6.23, v_num=bh2c, train_loss_step=6.640, train_acc_step=34.50, val_loss_step=6.760, val_acc_step=32.90, val_loss_epoch=6.280, val_acc_epoch=35.60, trai







Epoch 19:  38%|▍| 9/24 [00:07<00:11,  1.34it/s, loss=6.34, v_num=bh2c, train_loss_step=7.280, train_acc_step=28.60, val_loss_step=6.970, val_acc_step=31.40, val_loss_epoch=6.470, val_acc_epoch=35.00, trai




